{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smeared Data Reduction\n",
    "\n",
    "This notebook compiles reduced SANS and USANS data collected at the NIST Center for Neutron Research over two trips that occured in August and December of 2019. The USANS data is smeared, and therefore, is kept separate from the SANS data so that when fitting the data, separate smearing conditions can be applied to the appropriate datasets.\n",
    "\n",
    "Samples are indentified with a sample key, numbered 1-35, 40-43 for the August batch and 101-114, 201-206, 301-310, 401-410 and 501-506 for the December batch. The SANS data can be found in this supplemental information data reduction and analysis folder at the location `./sans/Aug2019_SANS` and `./sans/Dec2019_SANS`. Also included are csv files with additional information matching the sample keys to the sample names, as well as identifying the file numbers to the appropriate samples (in the August batch). Please see the main README file with additional information about all sections of this supplemental information data reduction and analysis folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bumps.names import *\n",
    "from bumps.fitters import fit\n",
    "\n",
    "import sasmodels\n",
    "\n",
    "from sasmodels.core import load_model\n",
    "from sasmodels.bumps_model import Model, Experiment\n",
    "from sasmodels.data import load_data, plot_data, empty_data1D\n",
    "from sasmodels.direct_model import DirectModel\n",
    "\n",
    "import sas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter out the following sample keys as the samples were not pressed for a sufficient amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [201,202,204,410,308]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading August 2019 SANS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "aug_dir = './Aug2019_SANS/'\n",
    "\n",
    "# 'SamplefileNumbers.csv' connects the samples to the appropriate files\n",
    "file_nos = pd.read_csv(aug_dir + 'SampleFileNumbers.csv')\n",
    "ng0_files = {}   # first configuration on VSANS instrument\n",
    "ng5_files = {}   # second configuration on VSANS instrument\n",
    "\n",
    "for index,row in file_nos.iterrows():\n",
    "    sam_no = int(row['Sample'])\n",
    "    ng0_files[sam_no] = 'sans' + str(row['NG0']) + '.ABS'\n",
    "    ng5_files[sam_no] = 'sans' + str(row['NG5']) + '.ABS'\n",
    "    \n",
    "# loading the sans data for each sample using the AnalySAS package\n",
    "sans_data = {}\n",
    "for key in ng0_files.keys():\n",
    "    ng0_data = np.loadtxt(aug_dir + ng0_files[key], skiprows=12)[:,0:4]\n",
    "    ng5_data = np.loadtxt(aug_dir + ng5_files[key], skiprows=12)[:,0:4]\n",
    "        \n",
    "    # remove significantly smeared data points at the cross-over between configurations\n",
    "    mask = np.where(ng5_data[:,2]/ng5_data[:,1] < 0.75)\n",
    "    ng5_data = ng5_data[mask, :][0]\n",
    "    \n",
    "    data = np.vstack((ng0_data, ng5_data))\n",
    "    \n",
    "    # filter out negative intensity (noise)\n",
    "    mask = np.where(data[:,1] > 0)\n",
    "    data = data[mask,:][0]\n",
    "    \n",
    "    # removes signficantly smeared data pionts at the cross-over between configurations\n",
    "    if key not in filters:\n",
    "        sans_data[key] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading December 2019 SANS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dir = './Dec2019_SANS/'\n",
    "sample_info = pd.read_csv(dec_dir + 'SampleFileNumbers.csv')\n",
    "\n",
    "files = [file for file in os.listdir(dec_dir) if file[-4:] == '.ABS']\n",
    "keys = [int(file[:3]) for file in files]\n",
    "\n",
    "for key, file in zip(keys, files):\n",
    "    data = np.loadtxt(dec_dir + file, skiprows=6)[:,0:4]\n",
    "    \n",
    "    # remove significantly smeared data points at the cross-over between configurations\n",
    "    mask = np.where(data[:,2]/data[:,1] < 0.75)\n",
    "    data = data[mask, :][0]\n",
    "    \n",
    "    # filter out negative intensity (noise)\n",
    "    mask = np.where(data[:,1] > 0)\n",
    "    data = data[mask,:][0]\n",
    "    \n",
    "    if key not in filters:\n",
    "        sans_data[key] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the smeared SANS data in txt files for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = './Smeared_Data_20200629'\n",
    "os.makedirs(save_loc, exist_ok=True)\n",
    "\n",
    "for key, data in sans_data.items():\n",
    "    \n",
    "    np.savetxt(save_loc + '/CMW' + str(key) + '_SANS.txt', data, header = '<X> <Y> <dY> <dX>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the smeared USANS data from both August and December batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usans_data = {}\n",
    "files = [x for x in os.listdir('./USANS_smeared') if 'CMW' in x]\n",
    "\n",
    "for file in files:\n",
    "    key = int(file.split('_')[0][3:])\n",
    "    data = np.loadtxt('USANS_smeared/' + file, skiprows=7)\n",
    "    data = data[:,0:3]\n",
    "    \n",
    "    # trimming the data to q-range collected and removing noise where intensity is negative\n",
    "    mask = np.where((data[:,0] > 0.00005) & (data[:,1] > 0))\n",
    "    data = data[mask,:][0]\n",
    "    \n",
    "    # removing noisy data where signal was weak above background\n",
    "    mask = np.where((data[:,2]*100/data[:,1]) < 75)\n",
    "    \n",
    "    # sample 35 (a polystyrene background) required additional trimming due to low signal\n",
    "    if (key == 35 or key == 501):\n",
    "        mask = np.where(((data[:,2]*100/data[:,1]) < 75) & (data[:,0]<1e-3))\n",
    "        \n",
    "    data = data[mask,:][0]\n",
    "\n",
    "    # removing filtered samples and 310 (not enough signal above background)\n",
    "    if key not in filters and key != 310:\n",
    "        usans_data[key] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the smeared SANS data in txt files for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in usans_data.items():\n",
    "    if key not in filters:\n",
    "        np.savetxt(save_loc + '/CMW' + str(key) + '_USANS.txt', data, header = '<X> <Y> <dY>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sasmodels2] *",
   "language": "python",
   "name": "conda-env-sasmodels2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
